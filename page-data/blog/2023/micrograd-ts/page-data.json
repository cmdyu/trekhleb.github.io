{"componentChunkName":"component---src-templates-post-tsx-content-file-path-src-posts-2023-micrograd-ts-index-mdx","path":"/blog/2023/micrograd-ts/","result":{"data":{"mdx":{"id":"2b89d79b-d2ca-5afb-97be-92a81da86b19","body":"\nI recently went through a very detailed and well-explained Python-based project/lesson by [karpathy](https://twitter.com/karpathy) which is called [micrograd](https://github.com/karpathy/micrograd). This is a tiny scalar-valued autograd engine and a neural net on top of it. [This video](https://www.youtube.com/watch?v=VMj-3S1tku0) explains how to build such a network from scratch.\n\nThe project above is, as expected, built on *Python*. For learning purposes, I wanted to see how such a network may be implemented in *TypeScript* and came up with a ðŸ¤– [micrograd-ts](https://github.com/trekhleb/micrograd-ts) repository (and also with a [demo](https://trekhleb.dev/micrograd-ts/) of how the network may be trained).\n\nTrying to build anything on your own very often gives you a much better understanding of a topic. So, this was a good exercise, especially taking into account that the whole code is just a [~200 lines](https://github.com/trekhleb/micrograd-ts/tree/main/micrograd/) of TS code with no external dependencies.\n\nThe [micrograd-ts](https://github.com/trekhleb/micrograd-ts) repository might be useful for those who want to get a basic understanding of how neural networks work, using a TypeScript environment for experimentation.\n\nWith that being said, let me give you some more information about the project.\n\n## Project structure\n\n- [micrograd/](https://github.com/trekhleb/micrograd-ts/tree/main/micrograd/) â€” this folder is the core/purpose of the repo\n  - [engine.ts](https://github.com/trekhleb/micrograd-ts/tree/main/micrograd/engine.ts) â€” the scalar `Value` class that supports basic math operations like `add`, `sub`, `div`, `mul`, `pow`, `exp`, `tanh` and has a `backward()` method that calculates a derivative of the expression, which is required for back-propagation flow.\n  - [nn.ts](https://github.com/trekhleb/micrograd-ts/tree/main/micrograd/nn.ts) â€” the `Neuron`, `Layer`, and `MLP` (multi-layer perceptron) classes that implement a neural network on top of the differentiable scalar `Values`.\n- [demo/](https://github.com/trekhleb/micrograd-ts/tree/main/demo/) - demo React application to experiment with the micrograd code\n  - [src/demos/](https://github.com/trekhleb/micrograd-ts/tree/main/demo/src/demos/) - several playgrounds where you can experiment with the `Neuron`, `Layer`, and `MLP` classes.\n\n## Micrograd\n\nSee the ðŸŽ¬ [The spelled-out intro to neural networks and back-propagation: building micrograd](https://www.youtube.com/watch?v=VMj-3S1tku0) YouTube video for the detailed explanation of how neural networks and backpropagation work. The video also explains in detail what the `Neuron`, `Layer`, `MLP`, and `Value` classes do.\n\nBriefly, the `Value` class allows you to build a computation graph for some expression that consists of scalar values.\n\nHere is an example of how the computation graph for the `a * b + c` expression looks like:\n\n![graph-1.png](./assets/graph-1.png)\n\nBased on the `Value` class we can build a `Neuron` expression `X * W + b`. Here we're simulating a dot-product of matrix `X` (input features) and matrix `W` (neuron weights):\n\n![graph-2.png](./assets/graph-2.png)\n\nOut of `Neurons`, we can build the `MLP` network class that consists of several `Layers` of `Neurons`. The computation graph in this case may look a bit complex to be displayed here, but a simplified version might look like this:\n\n![graph-3.png](./assets/graph-3.png)\n\nThe main idea is that the computation graphs above \"know\" how to do automatic back propagation (in other words, how to calculate derivatives). This allows us to train the MLP network for several epochs and adjust the network weights in a way that reduces the ultimate loss:\n\n![training-1.gif](./assets/training-1.gif)\n\nThe [following demo](https://trekhleb.dev/micrograd-ts/) illustrates the training process of the Multilayer perceptron against a set of dynamically generated \"circular\" data, where the inner circle has positive labels (1), and the outer circle has negative labels (-1). Once the network has been trained, we test it against a uniform range of data to build a prediction heatmap: the red area is where the model predicts negative values and the green area is where the model predicts positive values.\n\n![testing-1.gif](./assets/testing-1.gif)\n\n## Demo (online)\n\nTo see the online demo/playground, check the following link:\n\nðŸ”— [trekhleb.dev/micrograd-ts](https://trekhleb.dev/micrograd-ts)\n\n## Demo (local)\n\nIf you want to experiment with the code locally, follow the instructions below.\n\n### Setup\n\nClone the current repo locally.\n\nSwitch to the demo folder:\n\n```sh\ncd ./demo\n```\n\nSetup node v18 using [nvm](https://github.com/nvm-sh/nvm) (optional):\n\n```sh\nnvm use\n```\n\nInstall dependencies:\n\n```sh\nnpm i\n```\n\nLaunch demo app:\n\n```sh\nnpm run dev\n```\n\nThe demo app will be available at `http://localhost:5173/micrograd-ts`\n\n### Playgrounds\n\nGo to the [./demo/src/demos/](https://github.com/trekhleb/micrograd-ts/tree/main/demo/src/demos/) to explore several playgrounds for the `Neuron`, `Layer`, and `MLP` classes.\n\n---\n\nI hope, playing around with the micrograd-ts code above and watching the video from Karpathy will be helpful at least for some of you, learners.\n\n---\n\nðŸŽµ Here is a bit of music-driven MLP training\n\nWith a learning rate of `0.3` the training process is stable and converges to the local loss function minimum most of the time.\n\nWith a learning rate of `1`, you may see how the gradient descent sometimes skips the local minimum by doing the \"steps\" that are too large. The learning is unstable in this case.\n\n<iframe width=\"100%\" height=\"315\" src=\"https://www.youtube.com/embed/Oan0VOWUBYo\" title=\"Multilayer Perceptron Training\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>\n","fields":{"slug":"/blog/2023/micrograd-ts/"},"internal":{"contentFilePath":"/home/runner/work/trekhleb.github.io/trekhleb.github.io/src/posts/2023/micrograd-ts/index.mdx"},"frontmatter":{"title":"Micrograd TS","summary":"A TypeScript version of karpathy/micrograd. A tiny scalar-valued autograd engine and a neural net on top of it.","date":"07 August, 2023","cover":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#080808","images":{"fallback":{"src":"/static/1482efe655da4fc709537ded494e2661/df4c2/02-cover.png","srcSet":"/static/1482efe655da4fc709537ded494e2661/b9707/02-cover.png 750w,\n/static/1482efe655da4fc709537ded494e2661/96e3d/02-cover.png 1080w,\n/static/1482efe655da4fc709537ded494e2661/03d4b/02-cover.png 1366w,\n/static/1482efe655da4fc709537ded494e2661/df4c2/02-cover.png 1920w","sizes":"100vw"},"sources":[{"srcSet":"/static/1482efe655da4fc709537ded494e2661/4da20/02-cover.webp 750w,\n/static/1482efe655da4fc709537ded494e2661/c46d5/02-cover.webp 1080w,\n/static/1482efe655da4fc709537ded494e2661/ae2bb/02-cover.webp 1366w,\n/static/1482efe655da4fc709537ded494e2661/8e997/02-cover.webp 1920w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.46718750000000003}}}}}},"pageContext":{"slug":"/blog/2023/micrograd-ts/","frontmatter":{"title":"Micrograd TS","summary":"A TypeScript version of karpathy/micrograd. A tiny scalar-valued autograd engine and a neural net on top of it.","cover":"assets/02-cover.png","date":"2023-08-07T00:00:00.000Z"}}},"staticQueryHashes":[],"slicesMap":{}}